---
title: "xParse ETL + LangChain 构建实时智能解析 Agent"
description: "结合 xParse ETL 与 LangChain Agent，实现一体化的文档解析、知识入库与智能问答自动化流程。"
---

<Tip>
  本教程面向已经完成基础 Pipeline 与检索搭建的开发者，重点展示如何让 Agent 自动决定“是否需要解析新文档”“何时查询向量库”以及“如何组织最终回答”。
</Tip>

## 目标与适用场景

- **实时资料同步**：销售或法务团队把新版本材料放进对象存储后，Agent 可自动触发解析并更新知识库。
- **多轮智能问答**：同一会话里，Agent 灵活选择“运行 Pipeline”或“检索回答”工具，减少人工干预。
- **统一访问入口**：对业务系统暴露一个 HTTP API 或 Slack Bot，背后通过 LangChain Agent 编排多个能力。

## 典型业务案例

### 客服 Agent（Support Copilot）

- **资料来源**：FAQ、产品手册、工单复盘存放在 `s3://support-kb/faqs/`、`s3://support-kb/manuals/`。
- **Pipeline 关注点**：使用 `by_title` 策略保持章节语义，Milvus 集合命名为 `support_docs`，chunk 元数据需包含 `product_version` 便于 Agent 回复时引用。
- **Agent 行为**：当客服输入“有客户反馈 3.2.1 版本连接失败怎么处理？”时，Agent 先调用 `vector_search`；若检索不到最新版本，则触发 `run_xparse_client` 解析 `faqs/3.2.1/` 目录，之后再次检索并返回引用。

### 营销 Agent（Campaign Planner）

- **资料来源**：品牌素材、过往活动总结、竞品调研报告。建议在 S3 中按季度分文件夹，Agent 输入“拉取 Q4”时即可定位。
- **Pipeline 关注点**：使用 `semantic` 或 `by_page` 分块以保留长段创意描述；向量库额外存储 `audience`、`cta` 字段，便于 Agent 调整文案语气。
- **Agent 行为**：当用户说“为新品发布会写一段 30 秒广告词”时，Agent 先检索、拼接历史爆款素材，再结合业务 API（如投放预算）生成建议。

### 教育 Agent（Learning Companion）

- **资料来源**：课程 PPT、试题库、实验指导书，通常来自校园网盘或本地 NAS。
- **Pipeline 关注点**：开启 `include_orig_elements=True` 以保留表格/公式；若需要双语教学，可在 chunk metadata 写入 `language`。
- **Agent 行为**：学习者提问“请解释热力学第二定律并给出实验例子”，Agent 检索目标课程章节，再根据 `language` 字段决定回复语言或是否需要调用 `run_xparse_client` 同步最新课件。

为了方便在代码中切换不同场景，可以维护一份配置映射：

```python
from copy import deepcopy

SCENARIO_CONFIGS = {
    "support": {"source": {"prefix": "faqs/"}, "destination": {"collection_name": "support_docs"}},
    "marketing": {"chunk_config": {"strategy": "semantic", "max_characters": 1200}},
    "education": {"chunk_config": {"include_orig_elements": True}, "destination": {"collection_name": "course_docs"}}
}

def merge_dict(target, patch):
    for key, value in patch.items():
        if isinstance(value, dict) and isinstance(target.get(key), dict):
            merge_dict(target[key], value)
        else:
            target[key] = value
    return target

def get_pipeline_config(scene: str = "support"):
    cfg = deepcopy(PIPELINE_CONFIG)
    overrides = SCENARIO_CONFIGS.get(scene, {})
    return merge_dict(cfg, overrides)
```

> 如果场景差异很大，也可以把完整 YAML 配置存放在对象存储，由 Agent 根据用户意图动态加载。

## 总体架构

```
外部触发（Webhook / Chat UI）
        ↓
[LangChain Agent]
        ├─ Tool A：run_xparse_client
        ├─ Tool B：vector_search (Milvus/Zilliz)
        └─ Tool C：business_api（可选）
        ↓
最终回答（含引用与处理日志）
```

关键设计要点：

1. **Pipeline 配置模块化**：通过 `create_pipeline_from_config` 暴露统一入口，方便被 Tool 调用。
2. **状态记录**：Tool 返回执行摘要（耗时、任务 ID、入库条数），让 Agent 能在回答中说明“我刚解析了哪些文件”。
3. **一致的向量化模型**：聊天检索使用与 Pipeline 相同的 embedding 配置，保证语义空间一致。

## 环境准备

```bash
python -m venv .venv && source .venv/bin/activate
pip install "xparse-pipeline>=0.6" langchain langchain-community langchain-core \
            pymilvus qianfan # 或者 openai/aliyun 对应 SDK
export XTI_APP_ID=your-app-id
export XTI_SECRET_CODE=your-secret-code
export MILVUS_URI=./agent_vectors.db
```

建议把 API 密钥写进 `.env`，并使用 `python-dotenv` 在程序启动时加载。

## Step 1：配置 xParse Pipeline

```python
from xparse_client import create_pipeline_from_config

PIPELINE_CONFIG = {
    "source": {
        "type": "s3",
        "bucket": "agent-demo",
        "prefix": "contracts/",
        "aws_access_key_id": "...",
        "aws_secret_access_key": "...",
        "region_name": "ap-southeast-1"
    },
    "destination": {
        "type": "milvus",
        "db_path": "./agent_vectors.db",
        "collection_name": "agent_docs",
        "dimension": 1024
    },
    "api_base_url": "https://api.textin.com/api/xparse",
    "api_headers": {
        "x-ti-app-id": os.getenv("XTI_APP_ID"),
        "x-ti-secret-code": os.getenv("XTI_SECRET_CODE")
    },
    "chunk_config": {
        "strategy": "by_title",
        "new_after_n_chars": 480,
        "max_characters": 1500,
        "overlap": 80
    },
    "embed_config": {
        "provider": "qwen",
        "model_name": "text-embedding-v4"
    }
}

def run_pipeline(job_name: str) -> dict:
    pipeline = create_pipeline_from_config(PIPELINE_CONFIG)
    stats = pipeline.run(job_name=job_name)
    return {
        "job_name": job_name,
        "file_count": stats.processed_files,
        "chunk_count": stats.chunk_count,
        "duration": stats.duration_seconds
    }
```

> 提示：`pipeline.run` 返回的统计字段请以项目实际版本为准；若当前 SDK 未提供，可自行封装日志。

## Step 2：构建 LangChain Tools

```python
from langchain.tools import Tool
from langchain_community.vectorstores import Milvus
from langchain.embeddings import QianfanEmbeddings # 或自定义 provider

embedding = QianfanEmbeddings(model_name="text-embedding-v4")
vector_store = Milvus(
    embedding_function=embedding,
    collection_name="agent_docs",
    connection_args={"uri": os.getenv("MILVUS_URI")}
)

def pipeline_tool_fn(doc_hint: str) -> str:
    """LangChain Tool 包装：接受用户描述并触发 Pipeline"""
    result = run_pipeline(job_name=f"agent-job-{int(time.time())}")
    return f"已解析 {result['file_count']} 个文件，耗时 {result['duration']} 秒。"

def search_tool_fn(query: str) -> str:
    docs = vector_store.similarity_search(query, k=4)
    return "\n\n".join(f"[{i+1}] {doc.metadata.get('file_name')}\n{doc.page_content}"
                       for i, doc in enumerate(docs))

tools = [
    Tool(
        name="run_xparse_client",
        description="当需要获取/更新最新文档时使用，输入应是要解析的文档说明或目录。",
        func=pipeline_tool_fn
    ),
    Tool(
        name="vector_search",
        description="当需要基于已入库内容回答问题时使用，输入自然语言问题。",
        func=search_tool_fn
    )
]
```

## Step 3：配置 Agent

```python
from langchain.agents import initialize_agent, AgentType
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    temperature=0.2,
    api_key=os.getenv("OPENAI_API_KEY")
)

agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=True
)
```

> 若使用阿里、百炼等模型，可替换为相应的 LangChain 集成类，核心思路一致。

## Step 4：统一对话入口

```python
def chat_with_agent(query: str, session_id: str):
    response = agent.invoke({
        "input": query,
        "chat_history": history_store.get(session_id, [])
    })
    history_store.setdefault(session_id, []).append(
        (query, response["output"])
    )
    return response["output"]

print(chat_with_agent("新上传的合同有没有格式变化？"))
print(chat_with_agent("帮我总结违约条款，并引用来源。"))
```

“解析”与“检索”两个 Tool 会由 Agent 自动选择执行次数，开发者只需处理持久化的 `chat_history`。

## 最佳实践

- **幂等性**：为 `run_pipeline` 增加“最近一次执行时间”判断，避免 Agent 频繁重复解析同批文件。
- **引用输出**：把 `vector_search` 返回的 `metadata["page_id"]`、`source_url` 附在最终回答，增强可信度。
- **任务队列**：如果 Pipeline 耗时较长，可让 Tool 返回“任务已派发”，同时将任务写入队列，由 Worker 完成后更新状态。
- **监控**：记录 Agent 每次调用 Tool 的频率与时长，方便定位瓶颈（多数问题来自向量检索连接数不足）。

## 常见问题

- **Q：Agent 连续触发 Pipeline，如何限流？**  
  A：在 `pipeline_tool_fn` 里读取 Redis/数据库中的“last_run_at”，设置时间窗（例如 10 分钟）后拒绝调用。

- **Q：如何触发 Agent 解析指定文件？**  
  A：在 Tool 输入中约定 JSON，例如 `{"path":"contracts/2025Q1","priority":"high"}`，在函数里解析后传给 Pipeline。

- **Q：向量还没同步完就被检索？**  
  A：Milvus/Zilliz 默认写入即刻可读，但若开启了异步索引，建议在 Tool 返回前调用 `vector_store.client.flush()`。

完成以上步骤后，您即可把 Agent 集成到 IM 机器人、客服工作台或业务后台，为团队提供“能自己跑文档”的 AI 助理。


